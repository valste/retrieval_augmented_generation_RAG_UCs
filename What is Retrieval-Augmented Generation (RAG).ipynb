{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1048418",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# What RAG is (and why)\n",
    "\n",
    "RAG injects *fresh, external* knowledge into the model’s prompt at runtime: you retrieve relevant passages from your data, then ask ChatGPT to answer *using those passages*. This boosts factual accuracy and lets you ground responses in your own docs, wikis, tickets, PDFs, etc. ([OpenAI Platform][1], [OpenAI Help Center][2])\n",
    "\n",
    "---\n",
    "\n",
    "# The minimal architecture\n",
    "\n",
    "1. **Ingest & chunk** your content\n",
    "2. **Embed** chunks → vectors (use OpenAI Embeddings)\n",
    "3. **Index** vectors in a store (e.g., FAISS, pgvector, Pinecone)\n",
    "4. At query time: **embed the query → retrieve top-k** chunks\n",
    "5. **Augment the prompt** with retrieved chunks\n",
    "6. **Generate** the final answer with ChatGPT\n",
    "7. (Optional) **Re-rank**, **cite sources**, and **guardrail** (answer-only / refuse)\n",
    "\n",
    "OpenAI provides an embeddings API (e.g., `text-embedding-3-large` for max quality, or `...-3-small` for lower cost). ([OpenAI Platform][3], [OpenAI][4])\n",
    "\n",
    "---\n",
    "\n",
    "# Quick-start: Python (vanilla, no frameworks)\n",
    "\n",
    "> Installs: `pip install openai faiss-cpu tiktoken` (or use pgvector/Weaviate/etc. instead of FAISS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c697ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.102.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl.metadata (5.2 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from openai) (4.9.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from openai) (2.11.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\datascience\\env_dscourse\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.102.0-py3-none-any.whl (812 kB)\n",
      "   ---------------------------------------- 0.0/812.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 812.0/812.0 kB 5.9 MB/s  0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Downloading faiss_cpu-1.12.0-cp312-cp312-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/18.2 MB 4.8 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.1/18.2 MB 5.6 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 3.4/18.2 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.7/18.2 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.0/18.2 MB 6.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 7.3/18.2 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 8.4/18.2 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 9.7/18.2 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 11.0/18.2 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 12.3/18.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 13.6/18.2 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 14.9/18.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.3/18.2 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.6/18.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 6.1 MB/s  0:00:03\n",
      "Downloading tiktoken-0.11.0-cp312-cp312-win_amd64.whl (884 kB)\n",
      "   ---------------------------------------- 0.0/884.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 884.3/884.3 kB 5.7 MB/s  0:00:00\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: jiter, h11, faiss-cpu, distro, tiktoken, httpcore, httpx, openai\n",
      "\n",
      "  Attempting uninstall: h11\n",
      "\n",
      "    Found existing installation: h11 0.14.0\n",
      "\n",
      "    Uninstalling h11-0.14.0:\n",
      "\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "\n",
      "   ----- ---------------------------------- 1/8 [h11]\n",
      "   ---------- ----------------------------- 2/8 [faiss-cpu]\n",
      "   ---------- ----------------------------- 2/8 [faiss-cpu]\n",
      "   ---------- ----------------------------- 2/8 [faiss-cpu]\n",
      "   ---------- ----------------------------- 2/8 [faiss-cpu]\n",
      "   ---------- ----------------------------- 2/8 [faiss-cpu]\n",
      "   -------------------- ------------------- 4/8 [tiktoken]\n",
      "   ------------------------- -------------- 5/8 [httpcore]\n",
      "   ------------------------- -------------- 5/8 [httpcore]\n",
      "   ------------------------- -------------- 5/8 [httpcore]\n",
      "   ------------------------------ --------- 6/8 [httpx]\n",
      "   ------------------------------ --------- 6/8 [httpx]\n",
      "   ------------------------------ --------- 6/8 [httpx]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ----------------------------------- ---- 7/8 [openai]\n",
      "   ---------------------------------------- 8/8 [openai]\n",
      "\n",
      "Successfully installed distro-1.9.0 faiss-cpu-1.12.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jiter-0.10.0 openai-1.102.0 tiktoken-0.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install openai faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup\n",
    "from openai import OpenAI\n",
    "import faiss, numpy as np\n",
    "import textwrap, json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-large\"  # high accuracy; use -3-small to save cost\n",
    "\n",
    "def embed_texts(texts):\n",
    "    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)\n",
    "    return np.array([d.embedding for d in resp.data], dtype=\"float32\")\n",
    "\n",
    "# 2) Ingest & chunk (toy example)\n",
    "docs = {\n",
    "    \"doc1.md\": \"RAG augments LLMs with retrieval from external knowledge bases...\",\n",
    "    \"doc2.md\": \"Use embeddings to index text chunks; query → retrieve → prompt → answer.\"\n",
    "}\n",
    "def chunk(text, max_chars=800):\n",
    "    # simple splitter; replace with sentence-aware splitter in production\n",
    "    return textwrap.wrap(text, max_chars)\n",
    "\n",
    "chunks, meta = [], []\n",
    "for path, text in docs.items():\n",
    "    for i, ch in enumerate(chunk(text)):\n",
    "        chunks.append(ch)\n",
    "        meta.append({\"source\": path, \"chunk_id\": i})\n",
    "\n",
    "# 3) Build vector index\n",
    "vecs = embed_texts(chunks)\n",
    "index = faiss.IndexFlatIP(vecs.shape[1])\n",
    "# normalize for cosine similarity\n",
    "faiss.normalize_L2(vecs)\n",
    "index.add(vecs)\n",
    "\n",
    "# 4) Query → retrieve top-k\n",
    "def retrieve(query, k=4):\n",
    "    qv = embed_texts([query])\n",
    "    faiss.normalize_L2(qv)\n",
    "    scores, idxs = index.search(qv, k)\n",
    "    ctx = []\n",
    "    for rank, (i, s) in enumerate(zip(idxs[0], scores[0])):\n",
    "        ctx.append({\"text\": chunks[i], \"score\": float(s), **meta[i]})\n",
    "    return ctx\n",
    "\n",
    "# 5) Augment prompt & 6) Generate with ChatGPT\n",
    "CHAT_MODEL = \"gpt-4o-mini\"  # pick your chat model\n",
    "def answer(query):\n",
    "    ctx = retrieve(query)\n",
    "    context_block = \"\\n\\n\".join(\n",
    "        [f\"[{c['source']}#{c['chunk_id']}] {c['text']}\" for c in ctx]\n",
    "    )\n",
    "    system = \"You are a helpful assistant. Use ONLY the provided context. If missing, say you don't know.\"\n",
    "    user = f\"Question: {query}\\n\\nContext:\\n{context_block}\\n\\nAnswer with citations like [source#chunk].\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp.choices[0].message.content, ctx\n",
    "\n",
    "print(answer(\"What is RAG and how does it help?\")[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bf2813",
   "metadata": {},
   "source": [
    "**Why this works:** OpenAI embeddings turn text into vectors; nearest-neighbor search finds relevant chunks; you pass those chunks to ChatGPT in a tight prompt so it answers with grounded citations. ([OpenAI Platform][3])\n",
    "\n",
    "---\n",
    "\n",
    "# Quick-start: Node.js\n",
    "\n",
    "```js\n",
    "import OpenAI from \"openai\";\n",
    "const client = new OpenAI();\n",
    "\n",
    "const EMBED_MODEL = \"text-embedding-3-small\";\n",
    "\n",
    "async function embed(texts) {\n",
    "  const res = await client.embeddings.create({ model: EMBED_MODEL, input: texts });\n",
    "  return res.data.map(d => d.embedding);\n",
    "}\n",
    "\n",
    "// Use your vector DB client (e.g., pgvector) for upsert/query...\n",
    "// Then, at query time:\n",
    "\n",
    "async function ragAnswer(query, retrievedChunks) {\n",
    "  const context = retrievedChunks.map(c => `[${c.source}#${c.id}] ${c.text}`).join(\"\\n\\n\");\n",
    "  const system = \"Use only the provided context. If insufficient, say you don't know.\";\n",
    "  const user = `Q: ${query}\\n\\nContext:\\n${context}\\n\\nAnswer with citations.`;\n",
    "  const res = await client.chat.completions.create({\n",
    "    model: \"gpt-4o-mini\",\n",
    "    messages: [{ role: \"system\", content: system }, { role: \"user\", content: user }],\n",
    "    temperature: 0.2,\n",
    "  });\n",
    "  return res.choices[0].message.content;\n",
    "}\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cab01",
   "metadata": {},
   "source": [
    "# Good defaults & tips\n",
    "\n",
    "* **Chunking:** 400–1,000 characters or \\~200–500 tokens per chunk; overlap 10–20% to keep context intact. Sentence-aware splitting usually improves retrieval quality.\n",
    "* **Metadata:** Keep `source`, `section`, `created_at`, `tags`. You can filter by metadata before similarity search (e.g., only “policies”).\n",
    "* **Top-k:** Start with `k=4`–`8`. Tune via evaluation.\n",
    "* **Model choices:** Use `text-embedding-3-large` when quality matters, `...-3-small` for scale/cost. Any ChatGPT family chat model can do the generation step. ([OpenAI Platform][3], [OpenAI][4])\n",
    "* **Prompting:** Tell the model to (a) *quote or cite* chunks, (b) *refuse* when context is missing, and (c) *avoid adding facts not in context*. This aligns with OpenAI’s RAG best-practices. ([OpenAI Platform][1])\n",
    "* **Evaluation:** Create a small question set with gold answers; run A/B on chunk sizes, `k`, models, and prompts. OpenAI’s guide discusses accuracy optimization levers. ([OpenAI Platform][1])\n",
    "* **Security/PII:** Pre-filter or redact sensitive text before indexing. Add allow-lists by source.\n",
    "* **Caching:** Cache embeddings, retrieval results, and even full answers for repeated queries.\n",
    "* **Reranking (optional):** After initial vector search, call ChatGPT (or a smaller reranker) to score snippets *just* for relevance, then pass only the best 3–5 to the final prompt (improves precision on long corpora). ([OpenAI Platform][1])\n",
    "\n",
    "---\n",
    "\n",
    "# Variants with OpenAI features\n",
    "\n",
    "* **Assistants + Files / “file search”:** If you prefer a managed approach, the Assistants API has built-in retrieval on uploaded files—handy for quick prototypes and internal tools. (See “retrieval/file search” features in the OpenAI docs.) ([OpenAI Platform][1])\n",
    "* **Structured outputs:** Ask the model for JSON schemas (e.g., FAQ extraction) to power downstream UIs.\n",
    "\n",
    "---\n",
    "\n",
    "# When NOT to use RAG\n",
    "\n",
    "* Your task is *pure reasoning* on fully provided input (no external knowledge).\n",
    "* You control the schema tightly and need deterministic outputs → consider tool calls/functions or classical search.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, tell me:\n",
    "\n",
    "* your data sources (PDFs, Confluence, Git repos, Tickets),\n",
    "* your stack preference (FAISS/pgvector/Pinecone),\n",
    "* constraints (cost, latency, privacy),\n",
    "\n",
    "…and I’ll tailor a production-grade RAG plan (indexing scripts, schemas, eval harness) for your setup.\n",
    "\n",
    "[1]: https://platform.openai.com/docs/guides/optimizing-llm-accuracy/retrieval-augmented-generation-rag?utm_source=chatgpt.com \"OpenAI Guide: Optimizing LLM Accuracy with RAG\"\n",
    "[2]: https://help.openai.com/en/articles/8868588-retrieval-augmented-generation-rag-and-semantic-search-for-gpts?utm_source=chatgpt.com \"Retrieval Augmented Generation (RAG) and Semantic ...\"\n",
    "[3]: https://platform.openai.com/docs/guides/embeddings?utm_source=chatgpt.com \"OpenAI Embeddings Guide\"\n",
    "[4]: https://openai.com/index/new-embedding-models-and-api-updates/?utm_source=chatgpt.com \"New embedding models and API updates\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_dsCourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
